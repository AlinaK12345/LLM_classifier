{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Сначала размечаем данные train",
   "id": "745a5666a6712dfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:53:42.187471Z",
     "start_time": "2025-09-17T16:47:09.246528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "CATEGORIES = ['бытовая техника', 'обувь', 'одежда', 'посуда', 'текстиль',\n",
    "                  'товары для детей',\n",
    "                  'украшения и аксессуары',\n",
    "                  'электроника',\n",
    "                  'нет товара']\n",
    "\n",
    "\n",
    "\n",
    "print(\"Загружаем модель...\")\n",
    "model = pipeline(\"text-generation\", model=MODEL_NAME, device=-1)  # device=-1 для CPU\n"
   ],
   "id": "b6f2744a49b80cc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем модель...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.17it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Читаем данные...\n",
      "Будем размечать 5 отзывов\n",
      "Начинаем разметку...\n",
      "\n",
      "Готово! Общее время: 391.32 секунд\n",
      "Среднее время на отзыв: 78.2642 секунд\n",
      "Максимальное время: 95.8583 секунд\n",
      "Отзывов дольше 5 секунд: 5\n",
      "\n",
      "Распределение по категориям:\n",
      "category\n",
      "нет товара    3\n",
      "одежда        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T19:50:45.860070Z",
     "start_time": "2025-09-17T19:50:45.832424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INPUT_FILE = \"train.csv\"\n",
    "OUTPUT_FILE = \"ready_train.csv\"\n",
    "\n",
    "\n",
    "print(\"Читаем данные...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "reviews = df['text'].dropna().tolist()\n",
    "print(f\"Будем размечать {len(reviews)} отзывов\")"
   ],
   "id": "3d3ed9b7beebad37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Читаем данные...\n",
      "Будем размечать 1168 отзывов\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:31:23.478943Z",
     "start_time": "2025-09-17T19:51:08.253721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "KEYWORDS = {\n",
    "    \"бытовая техника\": [\n",
    "        \"холодильник\", \"стиральная\", \"посудомоечная\", \"микроволновка\", \"духовка\",\n",
    "        \"плита\", \"кофемашина\", \"чайник\", \"блендер\", \"миксер\", \"пылесос\",\n",
    "        \"фен\", \"мультиварка\", \"тостер\", \"вытяжка\", \"кондиционер\", \"обогреватель\"\n",
    "    ],\n",
    "    \"обувь\": [\n",
    "        \"туфли\", \"ботинки\", \"сапоги\", \"кроссовки\", \"кеды\", \"сандали\", \"босоножки\",\n",
    "        \"тапочки\", \"каблук\", \"подошва\", \"размер обуви\", \"мокасины\", \"лоферы\",\n",
    "        \"угги\", \"берцы\", \"шлепанцы\", \"зимняя обувь\", \"летняя обувь\"\n",
    "    ],\n",
    "    \"одежда\": [\n",
    "        \"футболка\", \"рубашка\", \"блузка\", \"свитер\", \"толстовка\", \"брюки\", \"джинсы\",\n",
    "        \"штаны\", \"шорты\", \"юбка\", \"платье\", \"пиджак\", \"куртка\", \"пальто\", \"пуховик\",\n",
    "        \"белье\", \"носки\", \"колготки\", \"спортивный костюм\", \"размер одежды\",\n",
    "         \"трусы\", \"бюстгальтер\", \"майка\", \"носки\", \"колготки\", \"чулки\",\n",
    "         \"леггинсы\", \"лосины\",\n",
    "\n",
    "        # Бренды и типы\n",
    "        \"джинсы\", \"свитшот\", \"топ\", \"боди\", \"кимоно\", \"туника\", \"бомбер\", \"парка\",\n",
    "        \"блейзер\", \"кардиган\", \"пончо\", \"кепи\", \"кепка\", \"шапка\", \"шарф\", \"перчатки\"\n",
    "    ],\n",
    "    \"посуда\": [\n",
    "        \"кастрюля\", \"сковорода\", \"тарелка\", \"чашка\", \"блюдо\", \"стакан\", \"кружка\",\n",
    "        \"нож\", \"вилка\", \"ложка\", \"сковородка\", \"ковш\", \"сотейник\", \"чайник\",\n",
    "        \"кофейник\", \"сервиз\", \"набор посуды\", \"столовые приборы\"\n",
    "    ],\n",
    "    \"текстиль\": [\n",
    "        \"полотенце\", \"простыня\", \"пододеяльник\", \"наволочка\", \"покрывало\", \"скатерть\",\n",
    "        \"салфетка\", \"шторы\", \"занавески\", \"ковер\", \"плед\", \"одеяло\", \"подушка\",\n",
    "        \"белье постельное\", \"матрас\", \"чехол\", \"габардин\", \"сатин\", \"хлопок\"\n",
    "    ],\n",
    "    \"товары для детей\": [\n",
    "        \"игрушка\", \"кукла\", \"машинка\", \"конструктор\", \"пазл\", \"коляска\", \"кроватка\",\n",
    "        \"пеленальный\", \"соска\", \"бутылочка\", \"подгузники\", \"питание детское\",\n",
    "        \"одежда детская\", \"обувь детская\", \"рюкзак школьный\", \"канцтовары\",\n",
    "        \"мобиль\", \"радионяня\"\n",
    "    ],\n",
    "    \"украшения и аксессуары\": [\n",
    "        \"кольцо\", \"серьги\", \"браслет\", \"цепочка\", \"кулон\", \"подвеска\", \"ожерелье\",\n",
    "        \"бижутерия\", \"сумка\", \"рюкзак\", \"кошелек\", \"ремень\", \"галстук\", \"часы\",\n",
    "        \"очки\", \"солнцезащитные\", \"зонт\", \"шапка\", \"шарф\", \"перчатки\"\n",
    "    ],\n",
    "    \"электроника\": [\n",
    "        \"ноутбук\", \"планшет\", \"компьютер\", \"монитор\",\n",
    "        \"телевизор\", \"наушники\", \"колонки\", \"фотоаппарат\", \"камера\", \"роутер\",\n",
    "        \"мышка\", \"клавиатура\", \"powerbank\", \"зарядное\", \"аккумулятор\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Начинаем разметку...\")\n",
    "results = []\n",
    "total_start = time.time()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    category = \"нет товара\"\n",
    "    review_lower = review.lower()\n",
    "\n",
    "\n",
    "    priority_categories = [\"одежда\", \"обувь\", \"украшения и аксессуары\"] + [cat for cat in CATEGORIES if cat not in [\"одежда\", \"обувь\", \"украшения и аксессуары\", \"нет товара\"]]\n",
    "\n",
    "    for cat in priority_categories:\n",
    "        if cat in KEYWORDS:\n",
    "            for keyword in KEYWORDS[cat]:\n",
    "                if keyword.lower() in review_lower:\n",
    "                    category = cat\n",
    "                    break\n",
    "            if category != \"нет товара\":\n",
    "                break\n",
    "\n",
    "\n",
    "    if category == \"нет товара\":\n",
    "        prompt = f\"Отнеси этот отзыв к одной из категорий ({', '.join(CATEGORIES)}): '{review[:100]}' Ответ:\"\n",
    "        response = model(prompt, max_new_tokens=4, temperature=0.1)\n",
    "        answer = response[0]['generated_text'].replace(prompt, \"\").strip()\n",
    "\n",
    "        for cat in CATEGORIES:\n",
    "            if cat.lower() in answer.lower():\n",
    "                category = cat\n",
    "                break\n",
    "\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    results.append({\n",
    "        \"text\": review,\n",
    "        \"category\": category,\n",
    "    })\n",
    "\n",
    "    # Сохраняем каждые 100 отзывов\n",
    "    if (i + 1) % 10 == 0:\n",
    "        df_temp = pd.DataFrame(results)\n",
    "        df_temp.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "        print(f\"Сохранено {i+1} отзывов\")\n",
    "\n",
    "    # Прогресс\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Обработано {i+1}/{len(reviews)} отзывов\")\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nГотово! Общее время: {total_time:.2f} секунд\")\n",
    "print(f\"Среднее время на отзыв: {df_results['processing_time'].mean():.4f} секунд\")\n",
    "print(f\"Максимальное время: {df_results['processing_time'].max():.4f} секунд\")\n",
    "\n",
    "print(\"\\nРаспределение по категориям:\")\n",
    "print(df_results['category'].value_counts())"
   ],
   "id": "f68582064f65bcda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем разметку...\n",
      "Обработано 10/1168 отзывов\n",
      "Обработано 20/1168 отзывов\n",
      "Обработано 30/1168 отзывов\n",
      "Обработано 40/1168 отзывов\n",
      "Обработано 50/1168 отзывов\n",
      "Обработано 60/1168 отзывов\n",
      "Обработано 70/1168 отзывов\n",
      "Обработано 80/1168 отзывов\n",
      "Обработано 90/1168 отзывов\n",
      "Сохранено 100 отзывов\n",
      "Обработано 100/1168 отзывов\n",
      "Обработано 110/1168 отзывов\n",
      "Обработано 120/1168 отзывов\n",
      "Обработано 130/1168 отзывов\n",
      "Обработано 140/1168 отзывов\n",
      "Обработано 150/1168 отзывов\n",
      "Обработано 160/1168 отзывов\n",
      "Обработано 170/1168 отзывов\n",
      "Обработано 180/1168 отзывов\n",
      "Обработано 190/1168 отзывов\n",
      "Сохранено 200 отзывов\n",
      "Обработано 200/1168 отзывов\n",
      "Обработано 210/1168 отзывов\n",
      "Обработано 220/1168 отзывов\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 88\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m category == \u001B[33m\"\u001B[39m\u001B[33mнет товара\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     87\u001B[39m     prompt = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mОтнеси этот отзыв к одной из категорий (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(CATEGORIES)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m): \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreview[:\u001B[32m100\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m Ответ:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m     response = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     89\u001B[39m     answer = response[\u001B[32m0\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mgenerated_text\u001B[39m\u001B[33m'\u001B[39m].replace(prompt, \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m).strip()\n\u001B[32m     91\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m cat \u001B[38;5;129;01min\u001B[39;00m CATEGORIES:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001B[39m, in \u001B[36mTextGenerationPipeline.__call__\u001B[39m\u001B[34m(self, text_inputs, **kwargs)\u001B[39m\n\u001B[32m    330\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m(\u001B[38;5;28mlist\u001B[39m(chats), **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m332\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1467\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1459\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1460\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1461\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1464\u001B[39m         )\n\u001B[32m   1465\u001B[39m     )\n\u001B[32m   1466\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1467\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1474\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1472\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m   1473\u001B[39m     model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m-> \u001B[39m\u001B[32m1474\u001B[39m     model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1475\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n\u001B[32m   1476\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1372\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1373\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1374\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1375\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1376\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001B[39m, in \u001B[36mTextGenerationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, **generate_kwargs)\u001B[39m\n\u001B[32m    429\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[32m    430\u001B[39m     generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.generation_config\n\u001B[32m--> \u001B[39m\u001B[32m432\u001B[39m output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, ModelOutput):\n\u001B[32m    435\u001B[39m     generated_sequence = output.sequences\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2539\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2528\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m GenerationMixin.generate(\n\u001B[32m   2529\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   2530\u001B[39m         inputs,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2534\u001B[39m         **kwargs,\n\u001B[32m   2535\u001B[39m     )\n\u001B[32m   2537\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001B[32m   2538\u001B[39m     \u001B[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2539\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2540\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2541\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2542\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2543\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2544\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2545\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2546\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2547\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2549\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2550\u001B[39m     \u001B[38;5;66;03m# 11. run beam sample\u001B[39;00m\n\u001B[32m   2551\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._beam_search(\n\u001B[32m   2552\u001B[39m         input_ids,\n\u001B[32m   2553\u001B[39m         logits_processor=prepared_logits_processor,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2557\u001B[39m         **model_kwargs,\n\u001B[32m   2558\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2867\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   2864\u001B[39m model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_hidden_states\u001B[39m\u001B[33m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[32m   2866\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[32m-> \u001B[39m\u001B[32m2867\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   2868\u001B[39m     is_prefill = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   2869\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:940\u001B[39m, in \u001B[36mcan_return_tuple.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    939\u001B[39m     return_dict = return_dict_passed\n\u001B[32m--> \u001B[39m\u001B[32m940\u001B[39m output = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    942\u001B[39m     output = output.to_tuple()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:465\u001B[39m, in \u001B[36mPhi3ForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    433\u001B[39m \u001B[38;5;129m@can_return_tuple\u001B[39m\n\u001B[32m    434\u001B[39m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[32m    435\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m   (...)\u001B[39m\u001B[32m    446\u001B[39m     **kwargs: Unpack[TransformersKwargs],\n\u001B[32m    447\u001B[39m ) -> CausalLMOutputWithPast:\n\u001B[32m    448\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    449\u001B[39m \u001B[33;03m    Example:\u001B[39;00m\n\u001B[32m    450\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    463\u001B[39m \u001B[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001B[39;00m\n\u001B[32m    464\u001B[39m \u001B[33;03m    ```\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m465\u001B[39m     outputs: BaseModelOutputWithPast = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    466\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    469\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    470\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    471\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    474\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    476\u001B[39m     hidden_states = outputs.last_hidden_state\n\u001B[32m    477\u001B[39m     \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001B[39m, in \u001B[36mcheck_model_inputs.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1061\u001B[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001B[32m   1062\u001B[39m                 monkey_patched_layers.append((module, original_forward))\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m outputs = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[38;5;66;03m# Restore original forward methods\u001B[39;00m\n\u001B[32m   1066\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module, original_forward \u001B[38;5;129;01min\u001B[39;00m monkey_patched_layers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:401\u001B[39m, in \u001B[36mPhi3Model.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001B[39m\n\u001B[32m    398\u001B[39m position_embeddings = \u001B[38;5;28mself\u001B[39m.rotary_emb(hidden_states, position_ids)\n\u001B[32m    400\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m decoder_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers[: \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers]:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m     hidden_states = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    405\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    407\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    408\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    409\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    410\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    411\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.norm(hidden_states)\n\u001B[32m    412\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m BaseModelOutputWithPast(\n\u001B[32m    413\u001B[39m     last_hidden_state=hidden_states,\n\u001B[32m    414\u001B[39m     past_key_values=past_key_values \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    415\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning_once(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:277\u001B[39m, in \u001B[36mPhi3DecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001B[39m\n\u001B[32m    275\u001B[39m residual = hidden_states\n\u001B[32m    276\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.post_attention_layernorm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m hidden_states = residual + \u001B[38;5;28mself\u001B[39m.resid_mlp_dropout(hidden_states)  \u001B[38;5;66;03m# main diff with Llama\u001B[39;00m\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:60\u001B[39m, in \u001B[36mPhi3MLP.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     up_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgate_up_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     62\u001B[39m     gate, up_states = up_states.chunk(\u001B[32m2\u001B[39m, dim=-\u001B[32m1\u001B[39m)\n\u001B[32m     63\u001B[39m     up_states = up_states * \u001B[38;5;28mself\u001B[39m.activation_fn(gate)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML_Sirius\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Генерация новых отзывов по категориям",
   "id": "5981130773717b3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Начинаем генерацию...\\n\")\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    results[cat] = []\n",
    "\n",
    "    for i in range(3):\n",
    "        prompt = f\"напиши короткий отзыв на товар с маркетплейса, товар категории: {cat} (1-2 предложения):\"\n",
    "        response = gen(prompt, max_new_tokens=25)\n",
    "\n",
    "        # Быстрая очистка ответа\n",
    "        review = response[0]['generated_text'].split(\":\")[-1].strip()\n",
    "\n",
    "\n",
    "        # results[cat].append(review)\n",
    "\n",
    "        df = pd.DataFrame([{'text': review, 'category' : cat}])\n",
    "        df.to_csv('ready_train.csv', index=False, encoding='utf-8', mode='a', header=False)\n",
    "\n",
    "print(\"ВСЕ ОТЗЫВЫ СГЕНЕРИРОВАНЫ!\")\n",
    "\n"
   ],
   "id": "7e92a6ee23a5e67f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Обучение модели на размеченных данных",
   "id": "a8eb824805444752"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import time\n",
    "\n",
    "# Включим автоматическое смешанная точность (AMP) для PyTorch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Разрешить TF32 для матричных умножений\n",
    "torch.backends.cudnn.allow_tf32 = True        # Разрешить TF32 для cuDNN\n",
    "\n",
    "\n",
    "df = pd.read_csv('ready_train.csv')\n",
    "print(f\"Загружено {len(df)} размеченных отзывов\")\n",
    "\n",
    "\n",
    "categories = CATEGORIES\n",
    "num_labels = len(categories)\n",
    "label2id = {label: idx for idx, label in enumerate(categories)}\n",
    "id2label = {idx: label for idx, label in enumerate(categories)}\n",
    "df['label'] = df['category'].map(label2id)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=torch.float16,  # Используем float16 вместо float32\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model = torch.compile(model)  # Компиляция модели для ускорения\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"weighted_f1\": f1_score(labels, predictions, average='weighted')}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,  # Увеличили batch size\n",
    "    per_device_eval_batch_size=64,   # Увеличили для валидации\n",
    "    num_train_epochs=2,              # Уменьшили количество эпох\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"weighted_f1\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "\n",
    "    fp16=True,                       # Автоматическое смешанная точность\n",
    "    tf32=True,                       # Использовать TensorFloat-32\n",
    "    dataloader_pin_memory=True,      # Закрепление памяти для быстрой загрузки\n",
    "    dataloader_num_workers=2,        # Многопоточная загрузка данных\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Начинаем обучение с оптимизацией...\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Обучение заняло: {end_time - start_time:.2f} секунд\")\n",
    "\n",
    "# 12. Сохранение оптимизированной модели\n",
    "trainer.save_model(\"./optimized_model\")\n",
    "tokenizer.save_pretrained(\"./optimized_model\")\n"
   ],
   "id": "b7cc84256c307ccd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
